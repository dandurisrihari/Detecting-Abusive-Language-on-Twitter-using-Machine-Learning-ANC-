{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Naive_Bayes.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOyjaLhiEade2sYSjbC0hrU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"id":"i4XP21B8v_B6","executionInfo":{"status":"error","timestamp":1607638405121,"user_tz":420,"elapsed":30392,"user":{"displayName":"Faiz Alam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMbHWyOyHalgu6WGcAs3QCu9MbMb9W9HNKMK9U=s64","userId":"17175696292390363784"}},"outputId":"a7cda5cf-d281-4217-f8f6-df0b9cba6e01"},"source":["#import all the necessary libarary\n","from __future__ import print_function\n","import os\n","import sys\n","import numpy as np\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount = True)\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout, Activation, GlobalAveragePooling1D\n","from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n","from keras.models import Model,Sequential\n","from keras.initializers import Constant\n","from matplotlib import pyplot\n","from keras import backend as K\n","import pandas as pd\n","from sklearn.utils import shuffle\n","from stop_words import get_stop_words\n","import string\n","import re\n","from sklearn.model_selection import train_test_split\n","from collections import Counter\n","from sklearn.metrics import f1_score\n","import time\n","#np.set_printoptions(threshold=sys.maxsize)\n","print('Indexing word vectors.')\n","MAX_SEQUENCE_LENGTH = 1000 #sequence length of the input vectors\n","MAX_NUM_WORDS = 20000 # Numbers of words that will be used to train the model\n","EMBEDDING_DIM = 100 #dimension of each of vector converted from each word\n","VALIDATION_SPLIT = 0.2 #validation size\n","embed_start = time.time() #test size\n","embeddings_index = {}\n","with open(\"/content/drive/My Drive/New_project/glove.txt\") as f: #location for glove embedding text file\n","    for line in f:\n","        word, coefs = line.split(maxsplit=1)\n","        coefs = np.fromstring(coefs, 'f', sep=' ')\n","        embeddings_index[word] = coefs\n","\n","print('Found %s word vectors.' % len(embeddings_index))\n","\n","df1=pd.read_csv('/content/drive/My Drive/New_project/hate.csv') #input the data set fro hate\n","df1.text=df1.text.astype(str)\n","df2=pd.read_csv('/content/drive/My Drive/New_project/neither.csv') #input the data set for neutral\n","df2.text=df2.text.astype(str)\n","df3=pd.read_csv('/content/drive/My Drive/New_project/offensive.csv') #input the dataset for offensive\n","df3.text=df3.text.astype(str)\n","df=pd.concat([df1,df2,df3], axis=0) #merge the dataset together\n","print(\"full data\",)\n","print(len(df))\n","df=shuffle(df) # shuffle the dataset\n","print(df.head())\n","# Define number of classes and number of tweets per class\n","n_class = 3\n","n_tweet = 16852\n","\n","# Divide into number of classes\n","if n_class == 2:\n","    df_pos = df.copy()[df.Annotation == 'hate'][:n_tweet]\n","    df_neg = df.copy()[df.Annotation == 'offensive'][:n_tweet]\n","    df_neu = pd.DataFrame()\n","    df = pd.concat([df_pos, df_neg], ignore_index=True).reset_index(drop=True)\n","elif n_class == 3:\n","    df_pos = df.copy()[df.Annotation == 'hate'][:n_tweet]\n","    df_neg = df.copy()[df.Annotation == 'offensive'][:n_tweet]\n","    df_neu = df.copy()[df.Annotation == 'neither'][:n_tweet]\n","    df = pd.concat([df_pos, df_neg, df_neu], ignore_index=True).reset_index(drop=True)\n","\n","# Define functions to process Tweet text and remove stop words\n","def ProTweets(tweet):\n","    tweet = ''.join(c for c in tweet if c not in string.punctuation)\n","    tweet = re.sub('((www\\S+)|(http\\S+))', 'urlsite', tweet)\n","    tweet = re.sub(r'\\d+', 'contnum', tweet)\n","    tweet = re.sub(' +',' ', tweet)\n","    tweet = tweet.lower().strip()\n","    return tweet\n","\n","def rmStopWords(tweet, stop_words):\n","    text = tweet.split()\n","    text = ' '.join(word for word in text if word not in stop_words)\n","    return text\n","\n","\n","# Get list of stop words\n","stop_words = get_stop_words('english')\n","stop_words = [''.join(c for c in s if c not in string.punctuation) for s in stop_words]\n","stop_words = [t.encode('utf-8') for t in stop_words]\n","\n","# Preprocess all tweet data\n","pro_tweets = []\n","for tweet in df['text']:\n","    processed = ProTweets(tweet)\n","    pro_stopw = rmStopWords(processed, stop_words)\n","    pro_tweets.append(pro_stopw)\n","embed_stop = time.time()\n","print(\"embedding generation\",(embed_stop-embed_start))\n","train_start = time.time()\n","X_train, X_test, y_train, y_test = train_test_split(df['text'], df['Annotation'], test_size=0.33, random_state=0)\n","\n","df_train = pd.DataFrame()\n","df_test = pd.DataFrame()\n","\n","df_train['text'] = X_train #creation of train dataset\n","df_train['Annotation'] = y_train\n","df_train = df_train.reset_index(drop=True)\n","\n","df_test['text'] = X_test #creation of test dataset\n","df_test['Annotation'] = y_test\n","df_test = df_test.reset_index(drop=True)\n","print('Training model.')\n","\n","# train a 1D convnet with global maxpooling\n","class TweetNBClassifier(object):\n","\n","    def __init__(self, df_train):\n","        self.df_train = df_train\n","        self.df_pos = df_train.copy()[df_train.Annotation == 'hate']\n","        self.df_neg = df_train.copy()[df_train.Annotation == 'offensive']\n","        self.df_neu = df_train.copy()[df_train.Annotation == 'neither']\n","\n","    def fit(self):\n","        Pr_pos = df_pos.shape[0]/self.df_train.shape[0]\n","        Pr_neg = df_neg.shape[0]/self.df_train.shape[0]\n","        Pr_neu = df_neu.shape[0]/self.df_train.shape[0]\n","        self.Prior  = (Pr_pos, Pr_neg, Pr_neu)\n","\n","        self.pos_words = ' '.join(self.df_pos['text'].tolist()).split()\n","        self.neg_words = ' '.join(self.df_neg['text'].tolist()).split()\n","        self.neu_words = ' '.join(self.df_neu['text'].tolist()).split()\n","\n","        all_words = ' '.join(self.df_train['text'].tolist()).split()\n","\n","        self.vocab = len(Counter(all_words))\n","\n","        wc_pos = len(' '.join(self.df_pos['text'].tolist()).split())\n","        wc_neg = len(' '.join(self.df_neg['text'].tolist()).split())\n","        wc_neu = len(' '.join(self.df_neu['text'].tolist()).split())\n","        self.word_count = (wc_pos, wc_neg, wc_neu)\n","        return self\n","\n","\n","    def predict(self, df_test): #prediction function\n","        class_choice = ['hate', 'offensive', 'neither'] #the three classes that has to be classified\n","\n","        classification = []\n","        for tweet in df_test['text']:\n","            text = tweet.split()\n","\n","            val_pos = np.array([])\n","            val_neg = np.array([])\n","            val_neu = np.array([])\n","            for word in text:\n","                tmp_pos = np.log((self.pos_words.count(word)+1)/(self.word_count[0]+self.vocab))\n","                tmp_neg = np.log((self.neg_words.count(word)+1)/(self.word_count[1]+self.vocab))\n","                tmp_neu = np.log((self.neu_words.count(word)+1)/(self.word_count[2]+self.vocab))\n","                val_pos = np.append(val_pos, tmp_pos)\n","                val_neg = np.append(val_neg, tmp_neg)\n","                val_neu = np.append(val_neu, tmp_neu)\n","\n","            val_pos = np.log(self.Prior[0]) + np.sum(val_pos)\n","            val_neg = np.log(self.Prior[1]) + np.sum(val_neg)\n","            val_neu = np.log(self.Prior[2]) + np.sum(val_neu)\n","\n","            probability = (val_pos, val_neg, val_neu) #probability calculation of each class\n","            classification.append(class_choice[np.argmax(probability)])\n","        return classification\n","\n","\n","    def score(self, feature, target):  #function to calculate the score\n","\n","        compare = []\n","        for i in range(0,len(feature)):\n","            if feature[i] == target[i]:\n","                tmp ='correct'\n","                compare.append(tmp)\n","            else:\n","                tmp ='incorrect'\n","                compare.append(tmp)\n","        r = Counter(compare)\n","        accuracy = r['correct']/(r['correct']+r['incorrect'])\n","        return accuracy\n","   \n","tnb = TweetNBClassifier(df_train)\n","tnb = tnb.fit()\n","train_stop = time.time() #calculating the train time\n","print(\"train_time\",(train_stop-train_start))\n","test_start =time.time()\n","predict = tnb.predict(df_test) #predicting the output of test tweet\n","#print(\"predict\",predict)\n","score = tnb.score(predict,df_test.Annotation.tolist())\n","train_accuracy = tnb.score(df_train.text.tolist(),df_train.Annotation.tolist()) #calculating the train accuracy\n","print(\"train_accuracy\",train_accuracy)\n","test_stop = time.time()\n","print(\"total_test_time\",(test_stop-test_start)) #calculating the test time\n","print(score)\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b222ed43d343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stop_words'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]}]}